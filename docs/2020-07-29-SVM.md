# Support vector machine



## 1. Margin of SVM



![support_vector_machine](..\img\svm\support_vector_machine.png)







$\textbf x^+$은 plus-plane 위에 있는 벡터이고, $\textbf x^-$는 minus-plane 위에 있는 벡터이다. 한편 plus-plane과 minus-plane은 서로 평행하며 $\textbf w$에 대해 수직이다. 이에 따라 $\textbf x^+$는 $\textbf x^-$를 $\textbf w$의 방향으로 $\lambda$만큼 평행 이동한 것이라고 할 수 있다. 이를 식으로 나타내면 아래와 같다.


$$
\begin{align}
\textbf x^+ = \textbf x^- + \lambda \textbf w
\end{align}
$$


이 식을 plus-plane에 대입하면 아래와 같이 $\lambda$를 나타낼 수 있다.


$$
\begin{align}
\textbf w^T \textbf x^+ + b &= 1 \\\\
\textbf w^T(\textbf x^- + \lambda \textbf w) + b &= 1 \\\\
\textbf  w^T \textbf x^- + \lambda \textbf w^T \textbf w + b &= 1 \\\\
-1 + \lambda \textbf w^T \textbf w &=1 \\\\
\lambda &= \frac{2}{\textbf w^T \textbf w}
\end{align}
$$


계속해서 $\textbf x^+$와 $\textbf x^-$ 사이의 거리 즉 마진(margin)은 다음과 같이 정의할 수 있다.


$$
\begin{align}
Margin &= distance(\textbf x^+, \textbf x^-) \\\\
&= ||\textbf x^+ - \textbf x^-||_2 \\\\
&= ||(\textbf x^- + \lambda \textbf w) - \textbf x^-||_2 \\\\
&= ||\lambda \textbf w||_2 \\\\
&= \frac{2}{\textbf w^T \textbf w} \sqrt{\textbf w^T \textbf w} \\\\
&= \frac{2}{\sqrt{\textbf w^T \textbf w}} \\\\
&= \frac{2}{||\textbf w||_2}
\end{align}
$$


## 2. Constrained optimization problem



이제 풀어야 하는 문제는 $margin$을 최대로 하는 $\textbf w$를 찾는 것이다. 이때 $\frac{2}{||\textbf w||_2}$를 최대화하는 문제는 $\frac{1}{2} ||\textbf w||_2$를 최소화하는 문제와 같다. 또한 margin 내부에는 어떠한 관측치도 없다는 조건을 $y_i (\textbf w^T \textbf x_i + b) \geq 1$로 나타낸다. 마지막으로 계산을 간소화하기 위해 $\frac{1}{2} ||\textbf w||_2$ 를 $\frac{1}{2} ||\textbf w||^2_2$로 나타낸다. 


$$
\begin{cases}
minimize \quad J(\textbf w) = \frac{1}{2} ||\textbf w||^2_2 \\\\
subject \ to \quad \ y_i(\textbf w^T \textbf x_i + b) \geq 1, i=1,2,...,N
\end{cases}
$$


위 문제를 라그랑제 승수(lagrange multiplier) $\boldsymbol \alpha$를 이용하면 다음과 같이 나타낼 수 있다.


$$
\begin{cases}
minimize \quad L(\textbf w, b, \boldsymbol \alpha) = 
\frac{1}{2}||\textbf w||^2_2 - \sum^N_{i=1} \alpha_i(y_i(\textbf w^T \textbf x_i + b) - 1) \\\\
subject \ to \quad \alpha_i \geq 0, i=1, 2, ..., N
\end{cases}
$$


## 3. KKT conditions and Lagrange dual problem



> 라그랑제 함수 $L(\boldsymbol \theta, \boldsymbol \lambda) = J(\boldsymbol \theta) - \sum^N_i \boldsymbol \lambda_i f_i(\boldsymbol \theta)$에 대한 KKT 조건
>
> (1) $\frac{\partial L(\boldsymbol \theta, \boldsymbol \lambda)}{\partial \boldsymbol \theta}=0$
>
> (2) $\boldsymbol \lambda_i \geq 0, i=1,2,...,N$
>
> (3) $\boldsymbol \lambda_i f_i(\boldsymbol \theta) = 0, i=1,2,...,N$



KKT (1) 조건에 의해 다음의 식을 도출할 수 있다.



- derivative of $L(\textbf w, b, \boldsymbol \alpha)$ with respect to $\textbf w$


$$
\begin{align}
\frac{\partial L(\textbf w, b, \boldsymbol \alpha)}{\partial \textbf w} = 0 \rightarrow 
\textbf w = \sum^N_{i=1} \alpha_i y_i \textbf x_i
\end{align}
$$


- derivative of $L(\textbf w, b, \boldsymbol \alpha)$ with respect to $b$


$$
\frac{L(\textbf w, b, \boldsymbol \alpha)}{\partial b} = 0 \rightarrow \sum^N_{i=1} \alpha_i y_i = 0
$$


위에서 도출한 두 개의 식을 이용해 최적화 문제를 아래와 같이 나타낼 수 있다.



1. $$
   \begin{align}
   \frac{1}{2}||\textbf w||^2_2 &= \frac{1}{2}\textbf w^T \textbf w \\\\
   &= \frac{1}{2} \textbf w^T \sum^N_{j=1} \alpha_j y_j \textbf x_j \\\\
   &= \frac{1}{2} \sum^N_{j = 1} \alpha_j y_j(\textbf w^T \textbf x_j) \\\\
   &= \frac{1}{2} \sum^N_{j = 1} \alpha_j y_j (\sum^N_{i = 1} \alpha_i y_i \textbf x_i^T \textbf x_j) \\\\
   &= \frac{1}{2} \sum^N_{i = 1} \sum^N_{j = 1} \alpha_i \alpha_j y_i y_j \textbf x_i^T \textbf x_j
   \end{align}
   $$

2. $$
   \begin{align}
   - \sum^N_{i=1} \alpha_i(y_i(\textbf w^T \textbf x_i + b) - 1) &= 
   - \sum^N_{i=1} \alpha_iy_i (\textbf w^T \textbf x_i + b) + \sum^N_{i=1}\alpha_i \\\\
   &= - \sum^N_{i=1} \alpha_iy_i \textbf w^T \textbf x_i - b \sum^N_{i=1} \alpha_i y_i + \sum^N_{i=1}\alpha_i \\\\
   &= \sum^N_{i=1}\alpha_i
   \end{align}
   $$

   


$$
\begin{cases}
maximize \quad L(\textbf w, b, \boldsymbol \alpha) = 
\sum^N_{i=1}\alpha_i - \frac{1}{2} \sum^N_{i = 1} \sum^N_{j = 1} \alpha_i \alpha_j y_i y_j \textbf x_i^T \textbf x_j \\\\
subject \ to \quad \sum^N_{i=1} \alpha_i y_i = 0 \\\\
\quad \quad \quad \quad \quad \ \ \alpha_i \geq 0, i=1, 2, ..., N
\end{cases}
$$


## 4. Solutions of SVM


$$
\textbf w = \sum^N_{i=1} \alpha_i y_i \textbf x_i
$$
 

이때 $\alpha_i$는 lagrange dual problem의 solution인데, 이 값이 0보다 클 경우에만 $\textbf w$를 찾는 데 영향을 미친다. 그 이유는 다음과 같다. 먼저  KKT (3) 조건에 의해 다음이 성립한다.


$$
\alpha_i(y_i(\textbf w^T \textbf x_i + b) - 1) = 0
$$


이것의 의미를 (1) $\alpha_i > 0$, (2) $\alpha_i = 0$으로 나누어 파악할 수 있다.



- $\alpha_i > 0 \quad and \quad y_i(\textbf w^T \textbf x_i + b) - 1 = 0$



$\textbf x_i$가 plus-plane 또는 minus-plane의 margin 위에 있다는 것을 의미한다. 즉 $\textbf x_i$는 support vector다.



- $\alpha_i = 0 \quad and \quad y_i(\textbf w^T \textbf x_i + b) - 1 \ne 0$



$\textbf x_i$가 margin 위에 있지 않다. 즉 hyperplane을 구하는 데 영향을 주지 않는다. SVM이 outlier에 robust한 이유이다.

